{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Pkg\n",
    "\n",
    "#Pkg.add(\"ImageIO\")\n",
    "using Flux\n",
    "using Flux: onehotbatch, onecold, crossentropy, @epochs\n",
    "\n",
    "using Images, ImageIO\n",
    "using Statistics\n",
    "\n",
    "using MLDatasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "println(readdir(\"data\"))\n",
    "println(readdir(\"data/train\"))\n",
    "#println(readdir(\"data/train/FAKE\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function load_cifake_data(base_path, split)\n",
    "    images = []\n",
    "    labels = []\n",
    "\n",
    "    # Map class names to numeric labels\n",
    "    class_map = Dict(\"FAKE\" => 0, \"REAL\" => 1)\n",
    "\n",
    "    # Construct the path for the split (train or test)\n",
    "    split_path = joinpath(base_path, split)\n",
    "\n",
    "    #println(\"Loading data from: \", split_path)\n",
    "\n",
    "    for (dirpath, dirs, files) in walkdir(split_path)\n",
    "        #println(\"Directory: \", dirpath)\n",
    "        for file in files\n",
    "            #println(\"Processing file: \", joinpath(dirpath, file))\n",
    "\n",
    "            # Load image\n",
    "            try\n",
    "                img = load(joinpath(dirpath, file))\n",
    "                img_array = Float32.(channelview(img))  # Convert RGB image to Float32 array\n",
    "                push!(images, img_array)\n",
    "            catch e\n",
    "                println(\"Error loading image: \", joinpath(dirpath, file), \" - \", e)\n",
    "                continue\n",
    "            end\n",
    "\n",
    "            # Extract label from directory name\n",
    "            class_name = basename(dirpath)\n",
    "            if haskey(class_map, class_name)\n",
    "                push!(labels, class_map[class_name])\n",
    "            else\n",
    "                #println(\"Warning: Unknown class name $class_name in path $dirpath\")\n",
    "            end\n",
    "        end\n",
    "    end\n",
    "\n",
    "    # Check if any images were loaded\n",
    "    if isempty(images)\n",
    "        println(\"No images were loaded. Check the dataset path and structure.\")\n",
    "        return [], []\n",
    "    end\n",
    "\n",
    "    # Convert to arrays\n",
    "    images = cat(images...; dims=4)  # Stack images along the 4th dimension\n",
    "    labels = onehotbatch(labels, 0:1)  # One-hot encode labels (0 for FAKE, 1 for REAL)\n",
    "\n",
    "    return images, labels\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "LoadError",
     "evalue": "InterruptException:",
     "output_type": "error",
     "traceback": [
      "InterruptException:",
      "",
      "Stacktrace:",
      " [1] _cat_size_shape(::NTuple{4, Bool}, ::NTuple{4, Int64}, ::Array{Float32, 3}, ::Array{Float32, 3}, ::Vararg{Array{Float32, 3}}) (repeats 843 times)",
      "   @ Base ./abstractarray.jl:1785",
      " [2] cat_size_shape(::NTuple{4, Bool}, ::Array{Float32, 3}, ::Array{Float32, 3}, ::Vararg{Array{Float32, 3}})",
      "   @ Base ./abstractarray.jl:1783",
      " [3] _cat_t(::Int64, ::Type{Float32}, ::Array{Float32, 3}, ::Vararg{Array{Float32, 3}})",
      "   @ Base ./abstractarray.jl:1824",
      " [4] _cat(::Int64, ::Array{Float32, 3}, ::Array{Float32, 3}, ::Vararg{Array{Float32, 3}})",
      "   @ Base ./abstractarray.jl:2086",
      " [5] cat(::Array{Float32, 3}, ::Vararg{Array{Float32, 3}}; dims::Int64)",
      "   @ Base ./abstractarray.jl:2084",
      " [6] load_cifake_data(base_path::String, split::String)",
      "   @ Main ./In[3]:45"
     ]
    }
   ],
   "source": [
    "base_path = \"data\"  # Base directory for the dataset\n",
    "\n",
    "# Load training data\n",
    "x_train, y_train = load_cifake_data(base_path, \"train\")\n",
    "println(\"x_train shape: \", size(x_train))  # Should be (H, W, C, N)\n",
    "println(\"y_train shape: \", size(y_train))  # Should be (num_classes, N)\n",
    "\n",
    "# Load testing data\n",
    "x_test, y_test = load_cifake_data(base_path, \"test\")\n",
    "println(\"x_test shape: \", size(x_test))  # Should be (H, W, C, N)\n",
    "println(\"y_test shape: \", size(y_test))  # Should be (num_classes, N)# Partition the dataset into chunks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_size = 1000  # Number of samples per chunk\n",
    "batch_size = 16    # Batch size for training within each chunk\n",
    "\n",
    "function partition_dataset(x, y, chunk_size)\n",
    "    n_samples = size(x, 4)  # Number of samples (last dimension)\n",
    "    chunks = []\n",
    "    for i in 1:chunk_size:n_samples\n",
    "        end_idx = min(i + chunk_size - 1, n_samples)\n",
    "        push!(chunks, (x[:, :, :, i:end_idx], y[i:end_idx]))\n",
    "    end\n",
    "    return chunks\n",
    "end\n",
    "\n",
    "\n",
    "chunks = partition_dataset(x_train, y_train, chunk_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a simple CNN.\n",
    "# With target size 32x32, after three 2×2 poolings, spatial dims become 4×4.\n",
    "model = Chain(\n",
    "    Conv((3,3), 3=>16, pad=1, relu),  # input has 3 channels\n",
    "    MaxPool((2,2)),\n",
    "    Conv((3,3), 16=>32, pad=1, relu),\n",
    "    MaxPool((2,2)),\n",
    "    Conv((3,3), 32=>64, pad=1, relu),\n",
    "    MaxPool((2,2)),\n",
    "    flatten,\n",
    "    Dense(64*4*4, 128, relu),\n",
    "    Dense(128, 2),\n",
    "    softmax\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the loss function.\n",
    "loss(x, y) = crossentropy(model(x), y)\n",
    "\n",
    "# Prepare the training data as a vector of tuples (for demonstration purposes).\n",
    "dataset = [(X[:,:,:,i], Y[:,i]) for i in 1:size(X,4)]\n",
    "\n",
    "# Use the ADAM optimizer.\n",
    "opt = ADAM()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in 1:epochs\n",
    "    println(\"Epoch $epoch starting...\")\n",
    "\n",
    "    for (x_chunk, y_chunk) in chunks\n",
    "        # Create a DataLoader for the current chunk\n",
    "        train_data = DataLoader((x_chunk, y_chunk), batchsize=batch_size, shuffle=true)\n",
    "\n",
    "        # Train on the current chunk\n",
    "        for (x, y) in train_data\n",
    "            grads = Flux.gradient(m -> loss(m(x), y), model)\n",
    "            Flux.update!(optimizer, model, grads)\n",
    "        end\n",
    "    end\n",
    "\n",
    "    println(\"Epoch $epoch complete\")\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model(X)\n",
    "acc = mean(onecold(predictions) .== onecold(Y))\n",
    "println(\"Training Accuracy: $(acc*100)%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.11.2",
   "language": "julia",
   "name": "julia-1.11"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
